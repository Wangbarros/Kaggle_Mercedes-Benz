{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = 'Data/'\n",
    "test_csv = 'test.csv'\n",
    "train_csv = 'train.csv'\n",
    "sample_csv = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(datapath,train_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variables with letters are categorical. Variables with 0/1 are binary values.')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_int = []\n",
    "tp_float = []\n",
    "tp_obj = []\n",
    "tp_other = []\n",
    "\n",
    "#Getting all variables names, less ID and looking for it types \n",
    "for i in data.columns.to_series()[1:]:\n",
    "    if data[i].dtype == 'int64':\n",
    "        tp_int.append(i)\n",
    "    elif data[i].dtype == 'float64':\n",
    "        tp_float.append(i)\n",
    "    elif data[i].dtype == 'object':\n",
    "        tp_obj.append(i)\n",
    "    else:\n",
    "        tp_other.append(i)\n",
    "        \n",
    "dic = {'tp_int': tp_int, 'tp_float': tp_float, 'tp_obj': tp_obj, 'tp_other': tp_other}\n",
    "\n",
    "print('Categorical:', tp_obj)\n",
    "print('Float:', tp_float)\n",
    "print('Has other type?', tp_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = []\n",
    "binarys = []\n",
    "for i in tp_int:\n",
    "    print('Variable: {}, Min: {}, Max: {} , Unique: {}'.format(i, data[i].min(), data[i].max() ,data[i].unique()))\n",
    "    if data[i].max() == 0:\n",
    "        drop.append(i)\n",
    "    elif data[i].max() == 1 and data[i].min() == 0 and len(data[i].unique()) == 2:\n",
    "        binarys.append(i)\n",
    "    else:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variables only with 0:', drop)\n",
    "\n",
    "if len(drop) + len(binarys) == len(tp_int):\n",
    "    print('All other integer variables are binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping variables only with zero\n",
    "data = data.drop(drop, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best way to work with categorical variables? I don't think is the best in all case. I will study about that\n",
    "\n",
    "for each in tp_obj:\n",
    "    dummies = pd.get_dummies(data[each], prefix=each, drop_first=False)\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "\n",
    "data = data.drop(tp_obj, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(os.path.join(datapath,test_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping variables only with zero\n",
    "data_test = data_test.drop(drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in tp_obj:\n",
    "    dummies = pd.get_dummies(data_test[each], prefix=each, drop_first=False)\n",
    "    data_test = pd.concat([data_test, dummies], axis=1)\n",
    "\n",
    "data_test = data_test.drop(tp_obj, axis=1)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping only columns who are in both data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep = np.intersect1d(data.columns.to_series(), data_test.columns.to_series())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[np.append(keep,'y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = data_test[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_y, test_y = model_selection.train_test_split(\n",
    "    data, data['y'], test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = train_features.drop(['ID', 'y'], axis = 1).values\n",
    "test_features = test_features.drop(['ID', 'y'], axis = 1).values\n",
    "\n",
    "train_y = train_y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "randomforest = RandomForestRegressor(n_estimators=200, max_features='auto', bootstrap=False, \n",
    "                                   oob_score=False, n_jobs=-1, random_state=0).fit(train_features, train_y)\n",
    "\n",
    "randomforest_score = randomforest.score(test_features, test_y)\n",
    "print('RF Score:', randomforest_score)\n",
    "\n",
    "predict = randomforest.predict(test_features)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(predict, test_y)\n",
    "print('R Square:', r_value**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = randomforest.predict(data_test.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ID': data_test['ID'], 'y': predict_test})\n",
    "submission.to_csv(os.path.join(datapath,'submissionrf.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# Define the neural network\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Inputs\n",
    "    net = tflearn.input_data([None, train_features.shape[1]])\n",
    "\n",
    "    # Hidden layer(s)\n",
    "    net = tflearn.fully_connected(net, 800, activation='ReLU')\n",
    "    net = tflearn.dropout(net, 0.80)  \n",
    "    \n",
    "    net = tflearn.fully_connected(net, 1200, activation='ReLU')\n",
    "    net = tflearn.dropout(net, 0.80)\n",
    "    \n",
    "    net = tflearn.fully_connected(net, 800, activation='ReLU')\n",
    "    net = tflearn.dropout(net, 0.80)\n",
    "\n",
    "    net = tflearn.fully_connected(net, 400, activation='ReLU')\n",
    "    net = tflearn.dropout(net, 0.80)    \n",
    "    \n",
    "    # Output layer and training model\n",
    "    net = tflearn.fully_connected(net, 1, activation='linear')\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss=\"mean_square\")\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "# Define the neural network\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Inputs\n",
    "    net = tflearn.input_data([None, train_features.shape[1]])\n",
    "\n",
    "    # Hidden layer(s)\n",
    "    net = tflearn.fully_connected(net, 512, activation='ReLU') \n",
    "    net = tflearn.fully_connected(net, 512, activation='ReLU')\n",
    "    net = tflearn.dropout(net, 0.80)\n",
    "    \n",
    "    # Output layer and training model\n",
    "    net = tflearn.fully_connected(net, 1, activation='linear')\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss=\"mean_square\")\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_features, train_y, validation_set=0.2, show_metric=True, batch_size=512, n_epoch=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_features)\n",
    "predict = [predict[i][0] for i in range(0, len(predict))]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(predict, test_y)\n",
    "print('R Square:', r_value**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = model.predict(data_test.iloc[:, 1:])\n",
    "predict_test = [predict_test[i][0] for i in range(0, len(predict_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ID': data_test['ID'], 'y': predict_test})\n",
    "print(submission.head())\n",
    "submission.to_csv(os.path.join(datapath,'submissiondp.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ()# mmm, xgboost, loved by everyone ^-^\n",
    "import xgboost as xgb\n",
    "\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = {\n",
    "    'n_trees': 500, \n",
    "    'eta': 0.005,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.95,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': train_y.mean(), # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(train_features, train_y)\n",
    "dtest = xgb.DMatrix(data_test.iloc[:, 1:].values)\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=500, # increase to have better results (~700)\n",
    "                   early_stopping_rounds=50,\n",
    "                   verbose_eval=50, \n",
    "                   show_stdv=False\n",
    "                  )\n",
    "\n",
    "num_boost_rounds = len(cv_result)\n",
    "print(num_boost_rounds)\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(dtrain.get_label(), model.predict(dtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and save results\n",
    "y_pred = model.predict(dtest)\n",
    "submission = pd.DataFrame({'id': data_test['ID'], 'y': y_pred})\n",
    "submission.to_csv(os.path.join(datapath,'submissionxg.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
